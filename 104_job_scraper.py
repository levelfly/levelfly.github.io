#!/usr/bin/env python3
"""
104äººåŠ›éŠ€è¡Œè·ç¼ºçˆ¬èŸ²
==================
é€™å€‹çˆ¬èŸ²å¯ä»¥æ ¹æ“šé—œéµå­—æœå°‹ 104 ä¸Šçš„è·ç¼ºï¼Œä¸¦å„²å­˜æˆ CSV æª”æ¡ˆã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    python 104_job_scraper.py

æ³¨æ„äº‹é …ï¼š
    1. è«‹é©ç•¶æ§åˆ¶çˆ¬å–é€Ÿåº¦ï¼Œé¿å…å°ç¶²ç«™é€ æˆè² æ“”
    2. æœ¬ç¨‹å¼åƒ…ä¾›å­¸ç¿’å’Œå€‹äººæ±‚è·ä½¿ç”¨
    3. è«‹éµå®ˆ 104 çš„ä½¿ç”¨æ¢æ¬¾
"""

import requests
import json
import csv
import time
import os
from datetime import datetime

class Job104Scraper:
    def __init__(self):
        self.base_url = "https://www.104.com.tw/jobs/search/list"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.104.com.tw/jobs/search/",
            "Accept": "application/json, text/plain, */*",
        }
        self.jobs = []

    def search_jobs(self, keyword, area=None, max_pages=5, delay=1.5):
        """
        æœå°‹è·ç¼º

        åƒæ•¸:
            keyword: æœå°‹é—œéµå­— (ä¾‹å¦‚: "Python", "å‰ç«¯å·¥ç¨‹å¸«")
            area: åœ°å€ä»£ç¢¼ (ä¾‹å¦‚: "6001001000" ç‚ºå°åŒ—å¸‚ï¼ŒNone ç‚ºå…¨å°ç£)
            max_pages: æœ€å¤šçˆ¬å–å¹¾é  (æ¯é ç´„ 20 ç­†)
            delay: æ¯æ¬¡è«‹æ±‚é–“éš”ç§’æ•¸ (å»ºè­° 1-2 ç§’)
        """
        print(f"\nğŸ” æœå°‹é—œéµå­—: {keyword}")
        print(f"ğŸ“„ é è¨ˆçˆ¬å–é æ•¸: {max_pages}")
        print("-" * 50)

        for page in range(1, max_pages + 1):
            params = {
                "ro": 0,           # 0: å…¨éƒ¨, 1: å…¨è·
                "kwop": 7,         # é—œéµå­—é¸é …
                "keyword": keyword,
                "order": 12,       # æ’åºæ–¹å¼: 12=æ—¥æœŸæ–°åˆ°èˆŠ
                "asc": 0,          # é™å†ª
                "page": page,
                "mode": "s",       # æœå°‹æ¨¡å¼
                "jobsource": "2018indexpoc",
            }

            if area:
                params["area"] = area

            try:
                response = requests.get(
                    self.base_url,
                    params=params,
                    headers=self.headers,
                    timeout=10
                )

                if response.status_code == 200:
                    data = response.json()
                    job_list = data.get("data", {}).get("list", [])

                    if not job_list:
                        print(f"âš ï¸  ç¬¬ {page} é æ²’æœ‰æ›´å¤šè³‡æ–™ï¼ŒçµæŸæœå°‹")
                        break

                    for job in job_list:
                        job_info = self._parse_job(job)
                        self.jobs.append(job_info)

                    total = data.get("data", {}).get("totalCount", "?")
                    print(f"âœ… ç¬¬ {page}/{max_pages} é å®Œæˆ | æœ¬é  {len(job_list)} ç­† | ç¸½å…±ç´„ {total} ç­†è·ç¼º")

                else:
                    print(f"âŒ ç¬¬ {page} é è«‹æ±‚å¤±æ•—: {response.status_code}")

            except Exception as e:
                print(f"âŒ ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤: {e}")

            # ç¦®è²Œæ€§å»¶é²
            if page < max_pages:
                time.sleep(delay)

        print(f"\nğŸ“Š ç¸½å…±çˆ¬å– {len(self.jobs)} ç­†è·ç¼º")
        return self.jobs

    def _parse_job(self, job):
        """è§£æå–®ä¸€è·ç¼ºè³‡æ–™"""
        return {
            "è·ç¼ºåç¨±": job.get("jobName", ""),
            "å…¬å¸åç¨±": job.get("custName", ""),
            "å·¥ä½œåœ°é»": job.get("jobAddrNo498", "") or job.get("jobAddress", ""),
            "è–ªè³‡": job.get("salaryDesc", ""),
            "å·¥ä½œç¶“é©—": job.get("periodDesc", ""),
            "å­¸æ­·è¦æ±‚": job.get("eduDesc", ""),
            "æ›´æ–°æ—¥æœŸ": job.get("appearDate", ""),
            "å·¥ä½œé€£çµ": f"https://www.104.com.tw/job/{job.get('link', {}).get('job', '')}" if job.get('link') else "",
            "å…¬å¸é€£çµ": f"https://www.104.com.tw/company/{job.get('link', {}).get('cust', '')}" if job.get('link') else "",
            "æ¨™ç±¤": ", ".join(job.get("tags", [])) if job.get("tags") else "",
        }

    def save_to_csv(self, filename=None):
        """å„²å­˜çµæœåˆ° CSV æª”æ¡ˆ"""
        if not self.jobs:
            print("âš ï¸  æ²’æœ‰è³‡æ–™å¯å„²å­˜")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"104_jobs_{timestamp}.csv"

        with open(filename, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=self.jobs[0].keys())
            writer.writeheader()
            writer.writerows(self.jobs)

        print(f"ğŸ’¾ å·²å„²å­˜è‡³: {filename}")
        return filename

    def display_sample(self, n=5):
        """é¡¯ç¤ºå‰ n ç­†è³‡æ–™"""
        print(f"\nğŸ“‹ å‰ {min(n, len(self.jobs))} ç­†è·ç¼ºé è¦½:")
        print("=" * 70)

        for i, job in enumerate(self.jobs[:n], 1):
            print(f"\n{i}. {job['è·ç¼ºåç¨±']}")
            print(f"   ğŸ¢ {job['å…¬å¸åç¨±']}")
            print(f"   ğŸ“ {job['å·¥ä½œåœ°é»']}")
            print(f"   ğŸ’° {job['è–ªè³‡']}")
            print(f"   ğŸ”— {job['å·¥ä½œé€£çµ']}")


# åœ°å€ä»£ç¢¼åƒè€ƒ
AREA_CODES = {
    "å°åŒ—å¸‚": "6001001000",
    "æ–°åŒ—å¸‚": "6001002000",
    "æ¡ƒåœ’å¸‚": "6001003000",
    "å°ä¸­å¸‚": "6001006000",
    "å°å—å¸‚": "6001010000",
    "é«˜é›„å¸‚": "6001011000",
    "æ–°ç«¹å¸‚": "6001004000",
    "æ–°ç«¹ç¸£": "6001004000",
}


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("ğŸ”§ 104 äººåŠ›éŠ€è¡Œè·ç¼ºçˆ¬èŸ²")
    print("=" * 60)

    scraper = Job104Scraper()

    # ===== è¨­å®šæœå°‹æ¢ä»¶ =====
    keyword = "Python"        # ä¿®æ”¹é€™è£¡ä¾†æœå°‹ä¸åŒé—œéµå­—
    area = None               # è¨­ç‚º None æœå°‹å…¨å°ï¼Œæˆ–ç”¨ AREA_CODES["å°åŒ—å¸‚"]
    max_pages = 3             # çˆ¬å–é æ•¸ (æ¯é ç´„ 20 ç­†)
    # ========================

    # é–‹å§‹çˆ¬å–
    jobs = scraper.search_jobs(
        keyword=keyword,
        area=area,
        max_pages=max_pages,
        delay=1.5  # æ¯é é–“éš” 1.5 ç§’
    )

    # é¡¯ç¤ºç¯„ä¾‹
    scraper.display_sample(5)

    # å„²å­˜åˆ° CSV
    scraper.save_to_csv()

    print("\nâœ… çˆ¬èŸ²åŸ·è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
